"""
Chatbot RAG avec Streamlit
Utilise paraphrase-multilingual-mpnet-base-v2 pour les embeddings
et google/flan-t5-base pour la g√©n√©ration
"""

import streamlit as st
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import faiss

# Configuration de la page
st.set_page_config(
    page_title="Chatbot RAG P√©dagogique",
    page_icon="ü§ñ",
    layout="wide"
)

# ============================================================
# INITIALISATION DES MOD√àLES (avec cache)
# ============================================================

@st.cache_resource
def load_embedding_model():
    """Charge le mod√®le d'embeddings - DIFF√âRENT du notebook prof"""
    model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    return SentenceTransformer(model_name)

@st.cache_resource
def load_generation_model():
    """Charge le mod√®le de g√©n√©ration - VERSION DIFF√âRENTE"""
    model_name = "google/flan-t5-base"  # Plus gros que small
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    return tokenizer, model, device

# ============================================================
# BASE DOCUMENTAIRE - Import√©e depuis documents.py
# ============================================================

try:
    from documents import get_documents, get_document_stats
    USE_EXTERNAL_DOCS = True
except ImportError:
    USE_EXTERNAL_DOCS = False
    
    def get_documents():
        """Base documentaire par d√©faut (fallback)"""
        return [
            """L'intelligence artificielle est une discipline scientifique qui vise √† cr√©er 
            des syst√®mes capables de r√©aliser des t√¢ches n√©cessitant normalement l'intelligence 
            humaine. Elle englobe plusieurs domaines comme l'apprentissage automatique, 
            le traitement du langage naturel et la vision par ordinateur.""",
            
            """Le machine learning est une branche de l'IA qui permet aux ordinateurs 
            d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s. 
            Les algorithmes de ML identifient des patterns dans les donn√©es pour faire 
            des pr√©dictions ou prendre des d√©cisions.""",
            
            """Le RAG (Retrieval-Augmented Generation) combine la recherche d'information 
            et la g√©n√©ration de texte. Le syst√®me r√©cup√®re d'abord des documents pertinents 
            puis g√©n√®re une r√©ponse bas√©e sur ces documents, ce qui am√©liore la fiabilit√©."""
        ]
    
    def get_document_stats():
        docs = get_documents()
        return {"count": len(docs), "total_words": 0, "avg_words_per_doc": 0}

# ============================================================
# FONCTIONS DE RETRIEVAL
# ============================================================

@st.cache_resource
def build_index(_embedder, documents):
    """Construit l'index FAISS √† partir des documents"""
    # Encoder les documents
    doc_embeddings = _embedder.encode(
        documents,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False
    )
    
    # Cr√©er l'index FAISS
    dim = doc_embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(doc_embeddings)
    
    return index, doc_embeddings

def search_documents(query, embedder, index, documents, top_k=3):
    """Recherche les documents les plus pertinents"""
    # Encoder la requ√™te
    query_emb = embedder.encode(
        [query],
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False
    )
    
    # Recherche dans FAISS
    scores, indices = index.search(query_emb, top_k)
    
    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx != -1:
            results.append({
                "index": int(idx),
                "score": float(score),
                "content": documents[idx]
            })
    
    return results

# ============================================================
# G√âN√âRATION DE R√âPONSE RAG
# ============================================================

def build_rag_prompt(question, retrieved_docs, history=None):
    """Construit le prompt pour le mod√®le g√©n√©ratif"""
    
    # Documents
    docs_text = ""
    for i, doc in enumerate(retrieved_docs):
        docs_text += f"{doc['content']}\n\n"
    
    # Prompt pour reformulation intelligente
    prompt = f"""Based on the following context, provide a clear and comprehensive answer in French.

Context:
{docs_text}

Question: {question}

Provide a natural, well-formulated answer in French that synthesizes the information:"""
    
    return prompt

def generate_answer(question, retrieved_docs, tokenizer, model, device, history=None, 
                   max_tokens=150, temperature=0.7, num_beams=4):
    """G√©n√®re une r√©ponse avec le mod√®le"""
    
    prompt = build_rag_prompt(question, retrieved_docs, history)
    
    # Tokenisation
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)
    
    # G√©n√©ration avec param√®tres optimis√©s pour r√©ponses longues
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            min_length=30,  # Force une r√©ponse minimale
            num_beams=num_beams,
            temperature=temperature,
            do_sample=True if temperature > 0 else False,
            top_p=0.95,
            repetition_penalty=1.1,
            length_penalty=1.5  # Favorise les r√©ponses plus longues
        )
    
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Si la r√©ponse du mod√®le est trop courte ou en anglais, utiliser les documents directement
    if len(answer.strip()) < 50 or not any(word in answer.lower() for word in ['le', 'la', 'est', 'les', 'un', 'une']):
        # Construire une r√©ponse √† partir des documents
        answer = ""
        for i, doc in enumerate(retrieved_docs[:2]):  # Utiliser les 2 meilleurs documents
            answer += doc['content'] + " "
        answer = answer.strip()
    
    return answer.strip()

# ============================================================
# INTERFACE STREAMLIT
# ============================================================

def main():
    st.title("ü§ñ Chatbot RAG P√©dagogique")
    st.markdown("### Assistant intelligent avec syst√®me de Retrieval-Augmented Generation")
    
    # Sidebar avec infos et param√®tres
    with st.sidebar:
        st.header("‚ÑπÔ∏è Informations")
        st.write("""
        **Mod√®les utilis√©s:**
        - Embeddings: paraphrase-multilingual-mpnet-base-v2
        - G√©n√©ration: flan-t5-base
        
        **Fonctionnalit√©s:**
        - ‚úÖ Recherche s√©mantique
        - ‚úÖ Index FAISS
        - ‚úÖ Historique de conversation
        - ‚úÖ R√©ponses bas√©es sur documents
        """)
        
        # Stats sur les documents
        stats = get_document_stats()
        st.divider()
        st.write("**üìö Base documentaire:**")
        st.write(f"- Documents: {stats['count']}")
        st.write(f"- Mots totaux: {stats['total_words']}")
        st.write(f"- Moyenne: {stats['avg_words_per_doc']} mots/doc")
        
        if USE_EXTERNAL_DOCS:
            st.success("‚úÖ Documents charg√©s depuis documents.py")
        else:
            st.warning("‚ö†Ô∏è Documents par d√©faut (cr√©er documents.py)")
        
        st.divider()
        
        # ============================================================
        # PARAM√àTRES R√âGLABLES
        # ============================================================
        st.header("‚öôÔ∏è Param√®tres RAG")
        
        st.subheader("Retrieval")
        top_k = st.slider(
            "Nombre de documents √† r√©cup√©rer",
            min_value=1,
            max_value=10,
            value=3,
            help="Nombre de documents les plus similaires √† utiliser"
        )
        
        similarity_threshold = st.slider(
            "Seuil de similarit√© minimum",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.05,
            help="Score minimum pour consid√©rer un document (0 = tous)"
        )
        
        st.subheader("G√©n√©ration")
        max_tokens = st.slider(
            "Longueur maximale de r√©ponse",
            min_value=50,
            max_value=500,
            value=200,  # Augment√© de 150 √† 200
            step=10,
            help="Nombre maximum de tokens g√©n√©r√©s"
        )
        
        temperature = st.slider(
            "Temp√©rature",
            min_value=0.1,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="Contr√¥le la cr√©ativit√© (bas = pr√©cis, haut = cr√©atif)"
        )
        
        num_beams = st.slider(
            "Beam Search",
            min_value=1,
            max_value=10,
            value=4,
            help="Nombre de branches pour la g√©n√©ration"
        )
        
        st.divider()
        
        if st.button("üóëÔ∏è R√©initialiser la conversation"):
            st.session_state.messages = []
            st.session_state.history = []
            st.rerun()
    
    # Initialisation de la session
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "history" not in st.session_state:
        st.session_state.history = []
    
    # Chargement des mod√®les (avec spinner)
    with st.spinner("Chargement des mod√®les..."):
        embedder = load_embedding_model()
        tokenizer, gen_model, device = load_generation_model()
        documents = get_documents()
        index, doc_embeddings = build_index(embedder, documents)
    
    # Affichage de l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # Afficher les documents sources si disponibles
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("üìö Documents sources"):
                    for i, doc in enumerate(message["sources"]):
                        st.markdown(f"""
                        **Document {i+1}** (score: {doc['score']:.3f})
                        
                        {doc['content'][:200]}...
                        """)
    
    # Input utilisateur
    if prompt := st.chat_input("Posez votre question..."):
        
        # Afficher le message utilisateur
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G√©n√©rer la r√©ponse
        with st.chat_message("assistant"):
            with st.spinner("Recherche et g√©n√©ration de la r√©ponse..."):
                
                # Retrieval avec param√®tres
                retrieved = search_documents(
                    prompt, 
                    embedder, 
                    index, 
                    documents, 
                    top_k=top_k
                )
                
                # Filtrer par seuil de similarit√©
                retrieved = [doc for doc in retrieved if doc['score'] >= similarity_threshold]
                
                if not retrieved:
                    answer = "‚ùå Aucun document pertinent trouv√©. Essayez de reformuler votre question."
                    st.warning(answer)
                else:
                    # G√©n√©ration avec param√®tres
                    answer = generate_answer(
                        prompt,
                        retrieved,
                        tokenizer,
                        gen_model,
                        device,
                        st.session_state.history,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        num_beams=num_beams
                    )
                    
                    # Affichage de la r√©ponse avec style professionnel
                    st.markdown("### R√©ponse")
                    st.info(answer)
                    
                    # Sources
                    with st.expander(f"üìö Documents sources utilis√©s ({len(retrieved)} documents)"):
                        for i, doc in enumerate(retrieved):
                            st.markdown(f"""
                            **üìÑ Document {i+1}** - Score de similarit√©: `{doc['score']:.3f}`
                            
                            {doc['content']}
                            """)
                            st.divider()
                
                # Mise √† jour de l'historique
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": answer,
                    "sources": retrieved if retrieved else []
                })
                st.session_state.history.append((prompt, answer))

if __name__ == "__main__":
    main()